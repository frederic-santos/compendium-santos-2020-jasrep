#+AUTHOR: Frédéric Santos
#+LATEX_CLASS: elsarticle
#+LATEX_CLASS_OPTIONS: [review, 3p]
#+OPTIONS: toc:nil author:nil
#+STARTUP: overview
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage[matha,mathb]{mathabx}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{lineno}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \journal{Journal of Archaeological Science: Reports}
#+LATEX_HEADER: \modulolinenumbers[1]
#+LATEX_HEADER: \bibliographystyle{model5-names}\biboptions{authoryear,sort}
#+LATEX_HEADER: \newcommand{\med}{\text{med}}
#+LANGUAGE: en

* Reviewers recommended to the editor                              :noexport:
- Gilles Escarguel
- Sébastien Lê
- Tamsin O'Connell
- Richard J. Smith
- Bruce E. Trumbo
* Initial cover letter                                             :noexport:
[[./cover_letter.org]]
* Shell command for reproducibility                                 :noexport:
  #+begin_src shell :eval no
  emacs -q -l init_Santos2020.el manuscript_outliers_Santos_2020.org
  #+end_src
* Frontmatter                                                        :ignore:
#+begin_export latex
\begin{frontmatter}

\title{Modern methods for old data: An overview of some robust methods for outliers detection with applications in osteology}

\author{Frédéric Santos\corref{cor1}}
\ead{frederic.santos@u-bordeaux.fr}
\cortext[cor1]{Corresponding author}
\address{Université de Bordeaux, UMR 5199 PACEA, Bâtiment B8, Allée Geoffroy Saint-Hilaire, CS 50023, 33615 Pessac Cedex, France.}

\begin{abstract}
Whereas outlier detection is routinely performed in archaeological sciences and may have a substantial impact on subsequent discussion and interpretations, modern and robust methods are rarely employed in our disciplinary field. The detection of univariate outliers mainly relies on the well-known rule of ``sample mean plus or minus two standard deviations'', whose the lack of robustness is illustrated in this article. Furthermore, specific and efficient methods for multivariate outliers seem to be very little known and rarely used through the literature published in the \textit{Journal of Archaeological Science: Reports}. To fill this gap, this article aims to present and summarize some robust methods well suited to the data usually gathered in archaeological and anthropological sciences, for both univariate and multivariate outliers. Robust methods for correlation and linear regression, whose results remain correct even in presence of strong outliers, are also illustrated. Methodological guidelines are discussed, in the light of applications in osteology. All the results (figures and tables) presented in this article can be fully reproduced with the companion R code available online, thus providing to the researchers some examples of templates for outliers detection.
\end{abstract}

\begin{keyword}
isolation forests \sep MAD \sep robust Mahalanobis distance \sep robust statistics \sep R language
\end{keyword}

\end{frontmatter}

\linenumbers
#+end_export
* Introduction
According to the intuitive definition formulated by [[citet:hawkins1980_IdentificationOutliers][p.1]], an outlier is "/an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism/". Detecting outliers is an important step, either upstream of statistical analyses or as a goal in its own right. Outliers may be due to various sources of error such as entry errors, strong measurement errors, or artifacts that may arise at different steps of data acquisition in virtual anthropology. But some outlying values may also reveal "true" anomalies in the data, and then bring important and relevant information, for they can contribute to identify pathological individuals citep:dietmeier2018_OxenOxonHill or individuals having too unusual values to be part of a given human group. The latter case is particularly frequent in isotopic studies where outliers (e.g. for $\delta{}^{13}C$, $\delta{}^{15}N$ or ${}^{87}Sr/{}^{86}Sr$ values) might indicate the presence of non-local individuals, thus allowing to discuss migrations and mobility patterns among human groups [[citep:santana-sagredo2015_IsotopicEvidenceDivergent,hakenbeck2010_DietMobilityEarly][e.g.,::]]. Outliers detection can thus have important consequences on subsequent interpretations.

When no pre-existing data providing a range of credible values for a given population can be used---which is the general case in archaeological sciences---this range of credible values must be estimated with statistical methods, traditionally using location and scale estimates calculated on the sample itself, that are supposed to accurately reveal the true parameters of the underlying population. The data used for those calculations thus include the potential outliers, which raises a crucial problem: if those location and scale estimates are non-robust, i.e. strongly influenced by the presence of outliers, they may fall far from the true population parameters, thus invalidating the whole procedure.

The handling of outliers often suffers from several problems and misuses in archaeological sciences. First, the number and identity of outliers may vary depending on the method employed citep:lightfoot2014_WaterConsumptionIron. Nonetheless, the method or criterion used to detect and identify outliers is not always explicitly specified in the scientific literature; this lack of precision is also frequent in other disciplinary subfields of social sciences citep:leys2013_DetectingOutliersNot. Second, the methods used in the literature are often not robust, and the decision rules used to identify outliers rely either on statistical indicators that are themselves imprecise in the presence of outliers, and/or a normality assumption which is not always clearly met [[citep:wright2005_IdentifyingImmigrantsTikal,webb2013_ExploringGeographicOrigins][e.g.,::]]. Finally, only a few publications utilize efficient and specific methods to detect multivariate outliers [[citep:harris1988_PrincipalComponentsAnalysis,mahoney2006_DentalMicrowearNatufian,algee-hewitt2016_PopulationInferenceContemporary][e.g.,::]]: both the modern statistical methods for detecting multivariate outliers and their implementation in free software seem to be little known. 

The problem of outlier detection is also closely related to the robustness of statistical methods. Indeed, outliers are often identified---and sometimes excluded---in search for a more ``representative'' sample to assess and discuss the correlation between two variables [[citep:loftus2012_TechnicalNoteInterpreting][e.g.,::]], or to build regression models [[citep:beck2019_DonThrowBaby][e.g.,::]]. This article will focus on robust methods, both for detecting outliers themselves, and also for getting more precise statistical estimates even when outliers are present in a dataset. Some simple examples where those robust methods outperform more classical and widely used methods will be given. The applications mainly consider osteological data here, but the methods presented will also suit any dataset including one or several continuous variables. Various population samples extracted from the Goldman Data Set freely available online citep:auerbach2004_HumanBodyMass will be used through the text, and a subsample from the reference sample of the DSP2 sex estimation method citep:bruzek2017_ValidationReliabilitySex will also be utilized in a case study.

The aim of this article is not to provide an exhaustive or practical in-depth review of all available methods of outlier detection. A comparison of several methods, applied on isotopic data, has recently been performed by cite:lightfoot2016_UseBiomineralOxygen in an enlightening article. cite:leys2019_HowClassifyDetect recently published a methodological note to "/fill the lack of an accessible overview of best practices/" (p. 1) for outliers detection in the field of psychology. However, there is a strong need that a similar dissemination reaches the field of archaeological sciences. For instance---as of april 2020---, a research within the database of the articles published in /Journal of Archaeological Science: Reports/ (https://www.sciencedirect.com/journal/journal-of-archaeological-science-reports/issues) triggered only four results for the keywords ``median absolute deviation'', three results for ``bagplot'', one result for ``robust Mahalanobis'', and no result could be found for requests about ``multivariate outliers'', ``isolation forests'' or the $S_n$ estimator. As concerns robust methods, no article matched the keywords ``robust regression'' or ``quantile regression''. Those results are nearly identical---and sometimes even lower---in other journals more oriented towards biological anthropology, such as the /American Journal of Physical Anthropology/ or the /International Journal of Osteoarchaeology/. Consequently, this article proposes a summary of the recent advances in statistics and provides ready-to-use R templates for modern methods of outlier detection.

Finally, to reinforce the move towards a reproducible research in archaeological and anthropological sciences citep:marwick2017_ComputationalReproducibilityArchaeological, this whole article has been written in Org-mode 9.3.6 for Emacs 26.3 citep:schulte2012_MultiLanguageComputingEnvironment and is fully reproducible with the org source file available on GitLab (\url{https://gitlab.com/f-santos/reproducibility-package-for-santos-2020-jasr}). Trying to follow the highest current standards of reproducibility citep:desquilbet2019_VersRechercheReproductible, a Docker image is also made available on DockerHub to provide the computational environment which allowed this study---full documentation on how using it can be retrieved from the GitLab repository. For a basic and simple re-use, the source codes of all tables and figures from this study are also available in separate R files. All the statistical analyses were performed with R 3.6.3 cite:rcoreteam2020_LanguageEnvironmentStatistical, and the list of R packages used is given in ref:appendix:r-packages.

* Univariate outliers
This first section deals with outlying values for one single variable. To present a concrete archaeological case, the left-right differences in humerus maximum length observed on the hunter-gatherers of Ipituaq (US-AK, 1500--1100 BP) are used. Those data are extracted from the Goldman Data Set online. This population sample is known to exhibit a substantial amount of asymmetry for this measurement citep:auerbach2008_PatternsClavicularBilateral. Since significant sex differences may be observed on the upper limbs for forager populations citep:weiss2009_SexDifferencesHumeral, only the 14 male individuals whose humeral length is known on both sides are considered. This small sample also permits the discussion of the robustness of the several methods presented below with the sample sizes usually available in archaeological sciences.

** The classical rule based on the sample mean and standard deviation
In biological anthropology, methods of outlier detection based on the mean and standard deviation are still frequently employed, including in recent research articles [[citep:bergstrom2019_NutritionalImportanceInvertebrates,lubritto2017_NewDietaryEvidence][e.g.,::]]. Any value out of the range defined by the mean plus or minus two or three standard deviation is then considered as an outlier. This criterion, also known as the "95–99.7 rule", is derived from the properties of the gaussian distribution: it is well known that about 95% and 99.7% of normally distributed values lie within two and three standard deviations from the mean respectively. This rule-of-thumb is both theoretically and practically correct when applied to a large enough sample for which the assumption of normality seems reasonable.

However, this method suffers from a critical lack of robustness in other situations, recently illustrated on real data from various disciplinary fields by cite:leys2013_DetectingOutliersNot and cite:lightfoot2016_UseBiomineralOxygen. The data sets handled in archaeological sciences do not always meet the previous requirements, or it may at least be difficult to check them because of their small sample size. When considering archaeological data, the sample mean and---above all---standard deviation may be drastically distorded by the presence of the extreme outliers themselves, and thus do not provide a good measure of distance to detect outliers.

#+begin_src R :results file graphics :file figures/failure2sd.png :exports results :width 600 :height 400 :tangle ./../R/Figure1_densityplot.R :session *R*
##############################
### Load required packages ###
##############################
library(anthrostat)
library(bioanth)

################################
### oad the Goldman Data Set ###
################################
data(goldman, package = "bioanth")
## Select the population sample from Ipituaq (males only):
dat <- subset(goldman, NOTE == 'Ipituaq - Point Hope, AK' & Sex == "M")

######################################################
### Compute left-right asymmetry in humeral length ###
######################################################
asym <- na.omit(dat$LHML - dat$RHML)
names(asym) <- 1:length(asym) # each individual is given a label

#########################################
### Density plot + outliers detection ###
#########################################
## Set graphical parameters:
par(cex = 1.15, mar = c(4.5, 4.5, 1, 1))
## Perform outliers detection with anthrostat R package:
id_outl <- norm_outliers(asym, coef = 2)
## Kernel density plot, with decision thresholds for outliers:
plot(id_outl, method = "mean_std", number_id = 2)
#+end_src

#+CAPTION: Kernel density estimation of the vector $x$ of left-right differences (in millimeters) in humeral length observed on the 14 male individuals from the population sample Ipituaq (US-AK, 1500--1100 BP) in the Goldman Data Set. The blue dotted vertical lines represent the exclusion thresholds defined by the classical rule based on the sample mean and standard deviation, equal to $\bar{x} \pm 2 \times \hat{\sigma}_x$. The third and eighth individuals are visual outliers. label:fig:failure2sd
#+ATTR_LATEX: :width 0.6\textwidth
#+RESULTS:
[[file:figures/failure2sd.png]]

#+begin_src R :results output :session *R* :exports none
## Compute some sample estimates (required for inline blocks below):
m <- mean(asym)
s <- sd(asym)
#+end_src

#+RESULTS:

Figure ref:fig:failure2sd provides an illustration of such a situation. The sample mean $\hat{\mu}$ = src_R[:results value latex :export results :session *R*]{round(m, 3)} {{{results(@@latex:-2.929@@)}}} and the standard deviation $\hat{\sigma}$ = src_R[:results value latex :export results :session *R*]{round(s, 3)} {{{results(@@latex:5.129@@)}}} are strongly inflated because of the two extreme values located on the right tail. The lack of robustness of the "mean plus or minus two standard deviations" decision rule is revealed by the failure to exclude one of the two outliers, since its value falls within the range $[\hat{\mu} - 2 \hat{\sigma}; \hat{\mu} + 2 \hat{\sigma}] =$ [src_R[:results value latex :export results :session *R*]{round(m-2*s, 3)} {{{results(@@latex:-13.186@@)}}} ; src_R[:results value latex :export results :session *R*]{round(m+2*s, 3)} {{{results(@@latex:7.329@@)}}}].

Albeit not artifical, the example presented here may be seen as peculiar, with a low sample size and two extreme values located on one single tail. However, it shows that this classical rule is clearly non-robust, and should only be used with much precaution and after a careful inspection of the data to ensure that the required assumptions are met.

** Robust alternatives for gaussian data
If the assumption of a normal $\mathcal{N}(\mu, \sigma^2)$ distribution of the data---disregarding some potential extreme values---seems to be reasonable for a given variable, several alternatives sharing the same philosophy do exist. All of them consist in using location and scale estimates for $\mu$ and $\sigma$ which are more robust than the classical sample choice of mean and standard deviation respectively. Consequently, the estimates calculated to define a "credible range of variation" outside of which any value can be considered as an outlier, are themselves less sensitive to the presence of outliers, thus always providing a more accurate estimation of the hidden population parameters.

For all the methods detailed in this section, the credible range of variation is defined by the following general formula, perfectly analagous to the "95-99.7 rule": 

#+begin_export latex
\begin{equation}
[m - k \cdot \hat{s} \, ; \, m + k \cdot \hat{s}] \label{eq:formula_loc_scale_univ}
\end{equation}
#+end_export

where $m$ is the sample median---a robust location estimate---, and $\hat{s}$ is a robust scale estimate citep:dorazio2017_OutlierDetectionRemarks. The choice of a constant $k$, usually lying between $2$ and $3$, allows to exclude only clear outliers (if set to a high value, since the interval will be wider) or even slightly suspicious values (if set to a low value, since the interval will be narrower), depending on the goals of the study and the type of data. When dealing with very small sample sizes, a conservative choice $k = 3$ might seem preferable to avoid false positives citep:leys2019_HowClassifyDetect. 

Among several choices for robust location estimates $\hat{s}$ proposed in statistical literature, three will be compared below: the interquartile range (IQR), the median absolute deviation (MAD), and the $S_n$ estimator---full mathematical details for each of them are available in ref:appendix:rob-scale-estimate. Those three estimators provide three different robust variants of formula \eqref{eq:formula_loc_scale_univ}, and therefore three acceptable decision rules for univariate outliers detection. To compare the results obtained with these variants to the results returned by the usual "95-99.7 rule", all four criteria were applied to the 14 male individuals from the Ipituaq population sample. The results can be found in Table ref:tab:comparison_loc_scale_methods.

#+begin_src R :results value table :exports results :colnames yes :rownames yes :tangle ./../R/Table1_compare_methods.R
##############################
### Load required packages ###
##############################
library(anthrostat)
library(bioanth)

#################################
### Load the Goldman Data Set ###
#################################
data(goldman, package = "bioanth")
## Select the population sample from Ipituaq (males only):
dat <- subset(goldman, NOTE == 'Ipituaq - Point Hope, AK' & Sex == "M")

######################################################
### Compute left-right asymmetry in humeral length ###
######################################################
asym <- na.omit(dat$LHML - dat$RHML)
names(asym) <- 1:length(asym) # each individual is given a label

#########################################################################
### Summarize and compare four different outlier detection strategies ###
#########################################################################
results <- norm_outliers(asym, coef = 2)
summary(results)
#+end_src

#+CAPTION: Comparison of four methods based on various location and scale estimates for outlier detection, applied on the data described in Figure ref:fig:failure2sd. "Coef" is the user-defined constant $k$ used for the construction of intervals, see equation \eqref{eq:formula_loc_scale_univ}. The lower and upper bounds of the intervals built with each method are indicated in the corresponding columns. The last column indicates the ID of the individuals flagged as outliers. label:tab:comparison_loc_scale_methods
#+RESULTS:
|                 | Location | Scale | Coef | Lower bound | Upper bound | Outliers |
|-----------------+----------+-------+------+-------------+-------------+----------|
| mean and sd     |   -2.929 | 5.129 |    2 |     -13.186 |       7.329 | 3        |
| median and IQR  |       -4 |  2.78 |    2 |       -9.56 |        1.56 | 3, 8     |
| median and MAD  |       -4 | 2.965 |    2 |       -9.93 |        1.93 | 3, 8     |
| median and $S_n$ |       -4 | 3.578 |    2 |     -11.156 |       3.156 | 3, 8     |

It can be seen that, unlike the usual method based on non-robust estimates, the three robust methods detect both the individuals 3 and 8 as outliers. None of them suffer from the inflation of location and scale estimates---caused by the two outliers located on the right tail---that affects the usual method. As a consequence, at any given value of $k$, the interval they provide for outlier detection is much narrower, and more accurately captures the range of usual values for the humeral asymmetry in this population sample.

** Robust methods which do not assume normality
In most contexts of archaeological sciences, such as osteometric or isotopic studies, there is almost always a presupposition of normality for all the variables considered---once again, discarding a few potential "true" outliers (e.g., migrants, pathological individuals or entry errors). As noted by [[citet:lightfoot2016_UseBiomineralOxygen][::p. 22]], skewed data may simply indicate a sample with several outliers on the same distribution tail, as in Figure ref:fig:failure2sd. 

Severely skewed distributions arise almost systematically in some disciplinary fields such as neurosciences citep:rousselet2019_ReactionTimesOther. Specific methods have been proposed for such variables, and numerous formulas do exist depending on the degree of skewness observed on the data citep:hubert2008_AdjustedBoxplotSkewed. Conversely, few variables studied by biological anthropologists or archaeologists are intrinsically far from normality. For those reasons, the need of specific methods for non-gaussian data is lower than in other disciplines. Consequently, the methods accounting for skewed distributions are to be used with caution, for they might lead to spurious results as it will be shown below.

As a general rule:
1. If the distribution may at least be considered as symmetric, the three robust variants exposed in section [[Robust alternatives for gaussian data]] remain valid, albeit more difficult to use since their scale factors (a specific constant required for the computations) must be approximated through computer simulations citep:rousseeuw1993_AlternativesMedianAbsolute.
2. If there is a good reason to suspect an asymmetric or skewed distribution in the whole underlying population, the use of a robust measure of skewness such as the medcouple citep:brys2004_RobustMeasureSkewness might constitute a useful first step. A high medcouple value (close to 1) may indicate that the variable is intrinsically skewed, i.e. exhibits a substantial skewness that is not only due to a few outliers.

In the general case of no particular assumption about the distribution of the variable, boxplot-based rules are a simple yet efficient way to proceed.

*** The classical boxplot rule
Boxplots are often used to detect univariate outliers. According to the standard boxplot rule citep:tukey1977_ExploratoryDataAnalysis, the credible range of credible values (i.e., the boxplot /fences/) is defined by:

#+begin_export latex
\begin{equation}
[q_1 - k \cdot IQR \, ; \, q_3 + k \cdot IQR] \label{eq:boxplot}
\end{equation}
#+end_export

where $q_1$ and $q_3$ are the first and third empirical quartiles respectively. The constant $k$ is traditionally set to $1.5$, although more conservative values such as 2 or 3 are also admissible depending on the goals of the study. It should be noted that this interval is centered around the arithmetic mean of $q_1$ and $q_3$ (which is usually not equal to the median) and is generally not symmetric.

This very general rule does not assume normality, but in the case of a large normal sample, about 0.35% of data points should be flagged as outliers at each end (i.e., 0.70% in total). However, this proportion may be different---much higher---in a symmetric heavy-tailed distribution.

*** Adjusted boxplots for skewed distributions
Some amendments to the previous rule have been proposed to achieve a better accuracy for skewed distributions. For slightly skewed distributions, cite:kimber1990_ExploratoryDataAnalysis proposed a rule based on so-called semi-interquartile ranges, and defined the following interval:

#+begin_export latex
\begin{equation}
[q_1 - 2k \cdot (m - q_1) \, ; \, q_3 + 2k \cdot (q_3 - m)]  \label{eq:adjusted_boxplot}
\end{equation}
#+end_export
using the notations previously introduced in equation \eqref{eq:boxplot}, and a value of $k$ still usually equal to 1.5.

*** Application to the Goldman Data Set
An example of visually slightly skewed distribution can be given by considering the asymmetry in tibia mediolateral diameter within the population sample of Giza (Egypt, 4700--4200 BP, shortcode in the Goldman Data Set: "Pyramiden, Gizeh"). A kernel density estimation of those values is presented in Figure ref:fig:asymGiza.

#+begin_src R :results file graphics :file figures/skewness.png :exports results :width 600 :height 400 :tangle ./../R/Figure2_Giza.R
##############################
### Load required packages ###
##############################
library(bioanth)
library(univOutl)

#################################
### Load the Goldman Data Set ###
#################################
data(goldman)
goldman <- as.data.frame(goldman) # tibble to data.frame
## Select the population sample of Giza:
dat <- subset(goldman, NOTE == "Pyramiden, Gizeh")

#########################################################
### Compute asymmetry in tibia medio-lateral diameter ###
#########################################################
dat <- na.omit(dat[ , c("RTMLD", "LTMLD")])
asym <- dat$RTMLD - dat$LTMLD
names(asym) <- 1:length(asym)

#########################################
### Density plot + outliers detection ###
#########################################
## Kernel density estimation:
kde <- density(asym, adjust = 1.4)
## Density plot:
par(cex = 1.15, mar = c(4.5, 4.5, 1, 1))
plot(kde, main = "")
rug(asym, col = "red", lwd = 2)
## Add the names of the most extreme values on the right tail:
text(x = sort(asym, dec = TRUE)[1:4], y = 0, pos = c(3, 4, 2, 3),
     labels = names(sort(asym, dec = TRUE)[1:4]), col = "red")
## Add thresholds for outlier detection:
abline(v = boxB(asym, method = "resistant")$fences, # standard fences
       col = "darkgoldenrod", lty = 2, lwd = 2)
abline(v = boxB(asym, method = "asymmetric")$fences, # asymmetric fences
       col = "purple", lty = 3, lwd = 2)
## Add a legend:
legend("topright", lty = c(2, 3),
       col = c("darkgoldenrod", "purple"),
       legend = c("Standard boxplot fences",
                  "Asymmetric boxplot fences")
       )
#+end_src

#+CAPTION: Kernel density estimation of the vector right-left differences (in millimeters) in tibial mediolateral diameter observed on the 21 individuals from the population sample of Giza (Egypt, 4700--4200 BP) in the Goldman Data Set. The four most extreme individuals on the right tail are labeled in red. label:fig:asymGiza
#+ATTR_LATEX: :width 0.6\textwidth
#+RESULTS:
[[file:figures/skewness.png]]

Out of any context, this distribution might simply be regarded as right-skewed, and asymmetric boxplot fences do not detect any outlier---not even the extreme individual 14. This basically means that /if one makes the assumption that tibial asymmetries are intrinsically right-skewed in the whole underlying population/, then no value can be regarded as an outlier in this sample. Such an asymmetry pattern might happen: as various subsets of a given population can present different degrees of directional asymmetry citep:graham2016_FluctuatingAsymmetryHuman, a complex mixture of fluctuating asymmetry, differential directional asymmetry and/or antisymmetry might indeed end in a skewed distribution. However, if this---strong---assumption is false, accounting for skewness leads to misleading results, since this skewness would not be a characteristic of the underlying population but rather a side-effect of several outliers located on the right tail. Indeed, standard boxplot fences (not adjusted for skewness) do detect the individual 14 as a clear outlier in this population sample.

Accounting for skewed distributions is then a delicate matter and relies on strong biological assumptions that should definitely be supported by previous knwoledge. The choice of a given method of outlier detection must not be based only on statistical considerations, but also depends on the biological knowledge about the variable and population studied citep:leys2019_HowClassifyDetect.

* Multivariate outliers
When several variables are involved, using specific methods is mandatory, and one should not rely only on a combination of univariate methods, although it may be a good starting point to get a basic understanding of the data citep:unwin2019_MultivariateOutliersO3. Among many other available algorithms such as ``Dbscan'' citep:ester1996_DensitybasedAlgorithmDiscovering or ``hdoutliers'' citep:wilkinson2018_VisualizingBigData, two methods are detailed below, which are both conceptually rather simple and practically easy-to-use, and have efficient implementations in both R and Python languages.

** Robust Mahalanobis distance
Unlike euclidean distance, Mahalanobis distance takes into account the correlation between the variables when computing dissimilarities among individuals. For this reason, it is popular in biological anthropology citep:pilloud2016_BiologicalDistanceAnalysis, where the data suffers almost always from intercorrelation. In a formal way, Mahalanobis distance between an individual $x_i$ (described by $p$ variables) and the multivariate sample mean $\hat{\mu}$ is defined by:

#+begin_export latex
\begin{equation}
D_{i} = \sqrt{{}^t(x_i - \hat{\mu}) \Sigma^{-1} (x_i - \hat{\mu})} \label{eq:maha}
\end{equation}
#+end_export

where $x_i, \hat{\mu} \in \mathbb{R}^p$, and $\Sigma$ is the $p \times p$ empirical covariance matrix.

The Mahalanobis distance can be used to detect multivariate outliers [[citep:stynder2009_CraniometricEvidenceSouth][e.g.,::]]. It is known to be primarily applicable to multivariate normal distributions---or at least elliptically symmetric unimodal distributions---although some studies suggest that its use can be generalized to some extent when the data depart from normality citep:warren2011_UseMahalanobisDistance. The outliers are those individuals whose the distance to the centroid $\hat{\mu}$ is greater than $\sqrt{\chi^2_{p; 1-\alpha}}$, i.e. the square-root of the $1-\alpha$ quantile of a Pearson distribution with $p$ degrees of freedom. $\alpha$ may usually vary from 0.001 (for a very conservative rule) to 0.05 (for a not too conservative rule), depending on the aim of the study.

This method is a generalization of the univariate rule relying on the sample mean and standard deviation, described in section [[The classical rule based on the sample mean and standard deviation]], and thus it suffers from the same lack of robustness. As for the ``95--99.7 rule'' in the univariate case, the estimates used in the formula \eqref{eq:maha} are non-robust and may be distorded by potential outliers, thus making invalid the whole decision rule.

A robust variant of Mahalanobis distance, also known as the MCD (minimum covariance determinant) algorithm, was proposed to circumvent these weaknesses citep:rousseeuw1999_FastAlgorithmMinimum,hubert2018_MinimumCovarianceDeterminant. Intuitively, it can be seen as an iterative method that uses only the "good part of the data" (i.e., uncontaminated data) to derive a robust location estimate $\hat{\mu}_{\text{MCD}}$ and a robust variability estimate $\hat{\Sigma}_{\text{MCD}}$ which will be used instead of the classical $\hat{\mu}$ and $\hat{\Sigma}$ estimates in equation \eqref{eq:maha}. As in the case of the classical Mahalanobis distance, the outliers are defined as those individuals whose robust Mahalanobis distance exceeds the threshold $\sqrt{\chi^2_{p; 1-\alpha}}$. More mathematical details, along with basic guidelines to determine the "good part of the data", are available in ref:appendix:robust-maha.

A simple (and easy to visualize) example may be used to illustrate the differences between the classical and robust versions of the Mahalanobis distance. Figure ref:fig:plot3d_Sayala represents a three-dimensional scatterplot for the Sayala population sample, retrieved from the Goldman Data Set. The maximal lengths of three long bones, the left femur, humerus and tibia, are considered. Visually, three outliers---the individuals 7, 14 and 20---can be identified.

#+begin_src R :results file graphics :file figures/plot3D-sayala.png :exports results :width 500 :height 450 :tangle ./../R/Figure3_plot3D_Sayala.R
##############################
### Load required packages ###
##############################
library(bioanth)
library(scatterplot3d)

#################################
### Load the Goldman Data Set ###
#################################
data(goldman, package = "bioanth")
## Select the population sample "Sayala":
sayala <- subset(goldman, NOTE == "Sayala")
## Select appropriate variables (left bones, 3 max. lengths):
sayala <- na.omit(sayala[ , c("LFML", "LTML", "LHML")])
## Relabel the individuals (more convenient in graphical representation):
rownames(sayala) <- 1:nrow(sayala)

###############
### 3D plot ###
###############
s3d <- scatterplot3d(x = sayala[, 1], y = sayala[, 2], z = sayala[, 3],
                     highlight.3d = TRUE, box = FALSE, type = "h",
                     pch = 16, lty.hplot = 3,
                     xlab = "LFML (mm)", ylab = "LTML (mm)", zlab = "LHML (mm)",
                     mar = c(2.5, 2.5, 0, 2))
text(s3d$xyz.convert(sayala), labels = rownames(sayala),
     pos = 3, cex = 0.9)
#+end_src

#+CAPTION: 3D scatterplot of the population sample of Sayala, drawn from the Goldman Data Set. The maximal lengths of three long bones are represented. label:fig:plot3d_Sayala
#+ATTR_LATEX: :width 0.55\textwidth
#+RESULTS:
[[file:figures/plot3D-sayala.png]]

The presence of those outliers causes an inflation of the generalized variance, i.e. a distorsion of the classical covariance matrix $\Sigma$. Consequently, the classical and robust Mahalanobis distances provide different sets of outliers here (Fig. ref:fig:stripcharts-maha). For an $\alpha$ level of 0.01, the classical version detects no outlier at all, whereas the robust version identifies the two individuals 14 and 20. For an $\alpha$ level of 0.05, the robust version also detects the individual 7, which is still far from the exclusion boundary for the classical version.

#+begin_src R :results file graphics :file figures/maha-dd.png :exports results :width 400 :height 400 :tangle ./../R/Figure4_stripcharts_mahalanobis.R
##############################
### Load required packages ###
##############################
library(bioanth)
library(robustbase)

#################################
### Load the Goldman Data Set ###
#################################
data(goldman, package = "bioanth")
goldman <- as.data.frame(goldman) # tibble to data.frame
## Select the population sample "Sayala" :
sayala <- subset(goldman, NOTE == "Sayala")
## Select appropriate variables (left bones, 3 max. lengths):
sayala <- na.omit(sayala[ , c("LFML", "LTML", "LHML")])
## Relabel the individuals (more convenient in graphical representation):
rownames(sayala) <- 1:nrow(sayala)

#####################################
### Compute Mahalanobis distances ###
#####################################
## Classic distance:
maha <- mahalanobis(sayala, center = colMeans(sayala),
                    cov = cov(sayala))
## Robust distances:
mcd <- covMcd(sayala, alpha = 0.75,
              nsamp = "best")$mah
## Add individual IDs:
names(mcd) <- names(maha) <- rownames(sayala)

#########################################################
### Plot the classic and robust Mahalanobis distances ###
#########################################################
set.seed(12345) # arbitrary seed to ensure reproducbility
par(cex = 1.15, mar = c(2.5, 4, 1, 1))
stripchart(x = list(maha, mcd), method = "jitter",
           vertical = TRUE, group.names = c("Classic", "Robust"),
           pch = 16, jitter = 0.04, ylab = "Mahalanobis distances")
## Add thresholds (Pearson quantiles):
abline(h = qchisq(0.99, df = 3), lty = 2, col = "red")
abline(h = qchisq(0.95, df = 3), lty = 2, col = "orange")
## Add the names of the individuals detected as outliers:
text(x = 2, y = sort(mcd, decreasing = TRUE)[1:3],
     labels = names(sort(mcd, decreasing = TRUE))[1:3], pos = 2)
text(x = c(0.95, 1.05), y = sort(maha, decreasing = TRUE)[1:2],
     labels = names(sort(maha, decreasing = TRUE))[1:2], pos = 3)
## Add the legend:
legend("topleft", lty = 2, col = c("red", "orange"),
       legend = c(expression(paste(alpha, " = ", 0.01)),
                  expression(paste(alpha, " = ", 0.05))))
#+end_src

#+CAPTION: Stripcharts displaying the squared classical and robust Mahalanobis distances between each individual and the centroid. The dotted lines symbolize the exclusion thresholds $\chi^2_{p;1-\alpha}$ for two different $\alpha$ values. The maximal lengths of three long bones from the population sample of Sayala (Goldman Data Set) were considered (LTML, LHML, LFML). label:fig:stripcharts-maha
#+ATTR_LATEX: :width 0.45\textwidth
#+RESULTS:
[[file:figures/maha-dd.png]]

However, even the robust Mahalanobis distance presents some drawbacks that are likely to be encountered in archaeological sciences. First, Mahalanobis distance can only capture linear relationships between variables, and can deliver spurious results when non-linear patterns are involved. Second, to achieve a sufficient stability and accuracy in the estimation of the covariance matrix, the number of individuals should be greater than three times the number of variables citep:harbottle1976_ActivationAnalysisArchaeology. Combining these two limitations, it is safer to use Mahalanobis distances only when dealing with a small number of dimensions. In such a situation, one can verify that there are no complex non-linear relationships in the data---for example using a pairs plot---, and it is easier to reach a sufficient sample size to ensure a reliable estimation of $\Sigma$.

** Isolation forests
Given the limitations of the classical procedures based on Mahalanobis distances, isolation forests present a useful and very robust alternative. Isolation forests are a recent algorithm of "anomaly detection" citep:liu2012_IsolationBasedAnomalyDetection, based on random forests citep:breiman2001_RandomForests. This method does not rely on any assumption about the distribution of the data, nor any given classical dissimilarity (e.g., euclidean, Mahalanobis).

The general idea is that "anomalies" can be defined by both their unusual values and their rarity, so that they are quite /isolated/ in the data, and therefore easy to localize. Indeed, identifying a point located right in the middle of a point cloud will usually require numerous instructions, whereas one single instruction may be sufficient to describe an outlier (e.g., "this is the only individual with $X_5 > 250$"). 

An isolation forest corresponds to a set of $B$ /isolation trees/, which are themselves randomly built decision trees that are grown until there is one single individual in each terminal leaf. Since outliers are supposed to be easily isolated in the data, they will correspond to the shortest paths in the isolation trees. A measure of credibility for an individual to be outlier is then its corresponding average path length within the $B$ isolation trees. An anomaly score, lying in $[0,1]$ and being a function of the sample size and the average path length, is computed for each individual.

According to cite:liu2012_IsolationBasedAnomalyDetection, a quick rule-of-thumb can provide a first indication as concerns the presence of outliers: if all the individuals have anomaly scores very close or inferior to 0.5, there is likely no multivariate outlier at all in the data. Conversely, if some anomaly scores depart from 0.5 and raise closer to 1, the corresponding individuals are likely to be outliers.

An isolation forest with 100 isolation trees is built on the same data as in the previous section (Sayala population sample with three variables: LTML, LHML, LFML). The anomaly scores, sorted by decreasing order, can be found in Figure ref:fig:anomaly_scores_sayala. The isolation forest algorithm provides evidence to consider the individuals 20, 7 and 14 as outliers, since their anomaly scores are the only ones to exhibit a substantial departure from the reference value of 0.50. This conclusion is consistent with the results obtained via the robust Mahalanobis distance (cf. Fig. ref:fig:stripcharts-maha). Isolation forests can thus provide a useful indication about possible multivariate outliers, by studying both the global distribution of anomaly scores (in search for "elbows" or gaps) and their absolute distance to 0.50.

#+begin_src R :results file graphics :file figures/anomaly_plot.png :exports results :width 650 :height 400 :tangle ./../R/Figure5_anomaly_scores_sayala.R
##############################
### Load required packages ###
##############################
library(bioanth)
library(FactoMineR)
library(solitude)

#################################
### Load the Goldman Data Set ###
#################################
data(goldman)
goldman <- as.data.frame(goldman) # tibble to data.frame
## Select the population sample "Sayala":
sayala <- subset(goldman, NOTE == "Sayala")
## Select three appropriate variables (max. lengths):
sayala <- na.omit(sayala[ , c("LFML", "LTML", "LHML")])
## Relabel the individuals:
rownames(sayala) <- 1:nrow(sayala)

#################################
### Build an isolation forest ###
#################################
isofo <- isolationForest$new(seed = 2020, nproc = 2,
                             sample_size = nrow(sayala),
                             num_trees = 100)
isofo$fit(sayala)
## Compute the anomaly scores:
scores <- round(isofo$scores, 3)
scores <- as.data.frame(scores[, c(1, 3)])
colnames(scores) <- c("ID", "anomaly_score")
## Sort the anomaly scores in decreasing order:
head(scores[order(scores$anomaly_score, decreasing = TRUE), ], 10)
ordered_scores <- scores[order(scores$anomaly_score, decreasing = TRUE), ]

###############################
### Plot the anomaly scores ###
###############################
par(cex = 1.21, mar = c(2, 4.5, 1, 1))
plot(x = 1:nrow(ordered_scores), y = ordered_scores$anomaly_score,
     type = "b", pch = 15, col = "navy", ylim = c(0.3, 0.85),
     xlab = "", ylab = "Anomaly score", axes = FALSE,
     main = "Anomaly scores by decreasing order")
## Add various decorations:
text(x = 1:nrow(ordered_scores), y = ordered_scores$anomaly_score,
     labels = ordered_scores$ID, pos = 3, col = "navy")
axis(side = 2)
abline(h = 0.5, lty = 2, col = "gray30")
#+end_src

#+CAPTION: Plot of the anomaly scores obtained by an isolation forest to detect outliers from the population sample of Sayala (Goldman Data Set), when three maximal lengths are considered (LTML, LHML, LFML). The scores are sorted in decreasing order and the corresponding individual IDs are indicated. label:fig:anomaly_scores_sayala
#+ATTR_LATEX: :width 0.6\textwidth
#+RESULTS:
[[file:figures/anomaly_plot.png]]

* Cellwise outliers: a case study
Although they may correspond to different situations, the two multivariate methods presented in section [[Multivariate outliers]] still have a common drawback. They allow an identification of the best candidates for being outliers, but they do not tell /why/ those individuals differ from the typical observations, i.e. on which variables they present anomalous values. Such an investigation is sometimes possible by inspecting several simple graphical outputs, such as a pairs plot---which is a matrix of pairwise bivariate scatterplots. However, this becomes very time-consuming and difficult when the number of variables increases. In such a case, one may think of principal component analysis as a way of finding the variables involved in the ``outlyingness'' of a given individual. But outliers may sometimes be visible only on the few last principal axes citep:jolliffe2002_PrincipalComponentAnalysis, which are usually not inspected; and they might even not be clearly visible at all citep:kandanaarachchi2019_DimensionReductionOutlier. Therefore, in some situations, it may be quite difficult to figure out what is different about an individual detected as suspect by the robust Mahalanobis distance or isolation forests.

This problem is addressed by a recent algorithm called DDC, for Deviating Data Cells citep:rousseeuw2018_DetectingDeviatingData. This algorithm seems to be particularly promising for osteoarchaeological studies, for it can handle missing values---to some extent---and allow rich and precise interpretations about the unusual measurements observed on an individual. In particular, this algorithm may allow to distinguish the individuals whose outlyingness is only due to their extremity on a single variable, and the individuals whose outlyingness is rather due to an unsual combination of values which would be perfectly acceptable when considered individually---i.e., ``shape outliers''.

DDC algorithm begins by finding potential extreme values on each single variable, and then looks for unusual combinations of values---e.g., a rather long femur and a rather short tibia---by considering subsets of correlated variables. All data cells exhibiting anomalies are /flagged/ in a graphical output: unusually low values are colored in blue, high values are colored in red, and all data cells presenting credible values are indicated in yellow. DDC therefore introduces a new paradigm in outlier detection, moving from /rowwise outliers/ (individuals globally considered as anomalies) to /cellwise outliers/ (each individual will usually have at most some flagged values, and still a bunch of credible values). One can also set the tolerance probability value, i.e. a cutoff value for flagging only extreme outliers or slightly unusual values (default value is 0.99).

This method can be illustrated on a subset of individuals extracted from the reference sample of DSP2. This subset is composed of 22 left ossa coxae belonging to male individuals from the Cleveland population sample. Following the DSP2 method, ten measurements have been collected on each os coxae, resulting in a small sample with only twice as many individuals than variables. With ten measurements, inspecting the 45 possible bivariate scatterplots is difficult and not necessarily informative, since the anomalies may imply combinations of four or more variables.

A PCA does not show any clear outliers on the first three principal axes. When considering each variable separately, only three individuals stand out according to the classical boxplot rule (extensive results available as Supporting Information online). The individual 96 exhibits a low value for the variable PUM, the individual 108 may be seen as an outlier for the variables for SPU and SS, and the individual 64 for the variables SS and VEAC. Unsurprisingly, those three individuals, being easy to "isolate" from the rest of the data, are the best candidates to be regarded as outliers according to the anomaly scores derived by isolation forests (Fig. ref:fig:anomaly_scores_dsp2).

#+begin_src R :results graphics file :file figures/detect_dsp2.png :exports results :width 600 :height 400 :session *R* :tangle ./../R/Figure6_anomaly_scores_dsp2.R
##############################
### Load required packages ###
##############################
library(anthrostat)
library(solitude)

######################
### Import dataset ###
######################
## Load DSP2 data:
data(data_dsp)
## Filter dataset:
dat <- subset(data_dsp, Collection == "Cleveland-EA")
dat <- subset(dat, Sex == "M")
dat <- subset(dat, Lat == "L")
dat <- na.omit(dat[, 5:ncol(dat)])

#############################
### Run iForest algorithm ###
#############################
isofo <- isolationForest$new(nproc = 3,
                             sample_size = nrow(dat),
                             num_trees = 1000)
isofo$fit(dat)
## Compute the anomaly scores:
scores <- round(isofo$scores, 3)
scores <- as.data.frame(scores[, c(1, 3)])
colnames(scores) <- c("ID", "anomaly_score")
## Sort the anomaly scores in decreasing order:
head(scores[order(scores$anomaly_score, decreasing = TRUE), ], 10)
ordered_scores <- scores[order(scores$anomaly_score, decreasing = TRUE), ]
## Plot the anomaly scores:
par(cex = 1.21, mar = c(1, 4.5, 1, 1))
plot(x = 1:nrow(ordered_scores), y = ordered_scores$anomaly_score,
     type = "b", pch = 15, col = "navy", ylim = c(0.35, 0.7),
     xlab = "", ylab = "Anomaly score", axes = FALSE,
     xlim = c(0, nrow(dat)),
     main = "Anomaly scores by decreasing order")
## Add various decorations:
text(x = 1:3, y = ordered_scores$anomaly_score[1:3],
     labels = rownames(dat)[ordered_scores$ID[1:3]], col = "navy",
     cex = 0.9, pos = c(3, 2, 4))
axis(side = 2)
abline(h = 0.5, lty = 2, col = "gray30")
#+end_src

#+CAPTION: Anomaly scores obtained with isolation forests for 22 male individuals extracted from the DSP2 reference sample. The three individuals with the highest anomaly scores are identified on the plot. label:fig:anomaly_scores_dsp2
#+ATTR_LATEX: :width 0.6\textwidth
#+RESULTS:
[[file:figures/detect_dsp2.png]]

However, this not entirely the end of the story: some unusual combinations of variables can also be observed on other individuals. Figure ref:fig:ddc_dsp2 shows the deviating data cells flagged by the DDC algorithm. The results already known from univariate analysis can usually also be retrieved on this plot: for instance, the individual 108 has indeed be flagged by the algorithm for having high values of SPU and SS, which confirms the results from the well known boxplot rule. However, many other cells are flagged, even for individual that show no univariate anomaly and have low anomaly scores in Figure ref:fig:anomaly_scores_dsp2. For instance, individual 76 exhibits an unusual combination of high PUM and low VEAC measurements: none of those values stand out by themselves but both are atypical with respect to the values taken by the variables most correlated to them. The individual 112 exhibits exactly the reverse combination, with low PUM and high VEAC values. Similarly, the individual 100 exhibits a combination of a rather high SS and very low SCOX, which is also unusual within this population sample. Those peculiarities can indeed be confirmed when going back to the raw data, but the PCA was totally unhelpful in identifying those slight anomalies. This highlights a crucial fact: when the anomaly only concerns one given pair of variables among ten possible measurements, the impact may be sufficiently moderate so that multivariate methods cannot consider the individual as /globally/ suspect. The DDC algorithm allows to detect the individuals having a slightly different morphology, even if it is restricted to a very precise region of the bone under study.

#+begin_src R :results graphics file :file figures/cellwise_dsp2.png :exports results :width 500 :height 600 :session *R* :tangle ./../R/Figure7_ddc_plot.R
##############################
### Load required packages ###
##############################
library(anthrostat)
library(cellWise)

######################
### Import dataset ###
######################
## Load DSP2 data:
data(data_dsp)
## Filter dataset:
dat <- subset(data_dsp, Collection == "Cleveland-EA")
dat <- subset(dat, Sex == "M")
dat <- subset(dat, Lat == "L")
dat <- na.omit(dat[, 5:ncol(dat)])

#########################
### Run DDC algorithm ###
#########################
ddc <- DDC(dat, DDCpars = list(tolProb = 0.975))
cellWise::cellMap(D = ddc$remX,
                  R = ddc$stdResid,
                  rowlabels = rownames(dat),
                  columnlabels = colnames(dat),
                  showVals = NULL)
#+end_src

#+CAPTION: Deviating data cells flagged by the DDC algorithm on 22 male individuals extracted from the DSP2 reference sample. Unusually low values are colored in blue (if strong anomaly) or puple (if slight), and high values are colored in red or orange. A tolerance probability of 0.975 has been used. label:fig:ddc_dsp2
#+ATTR_LATEX: :width 0.5\textwidth
#+RESULTS:
[[file:figures/cellwise_dsp2.png]]

* Bivariate outliers
This last section focuses on the particular case of bivariate data. Although general methods for multivariate outliers (especially the Mahalanobis distance, detailed in section [[Robust Mahalanobis distance]]) can also be used when considering only two variables, some tools were specifically developed for this situation.

** Outliers in the context of correlation and linear regression
When considering the relationship between two continuous variables, three main types of outliers can be defined. In the first panel of Figure ref:fig:type_outliers_reg, one single individual is far from the regression line, but its position---near the average of the explanatory variable RHML---gives it only a limited influence in the regression model. In the middle panel, two extreme individuals can be identified on the margins of the horizontal axis. However, those two individuals perfectly respect the relationship observed on the other individuals, and the regression lines with or without those two extreme points are indistinguishable. Finally, the right panel shows a /leverage/ individual, i.e. an individual which is both located on the margin of the explanatory variable and has a high residual value: this type of individual has a great influence in a regression model, especially when dealing with small sample sizes.

#+begin_src R :results file graphics :file figures/type_outliers_reg.png :exports results :width 900 :height 300 :tangle ./../R/Figure8_types_outliers.R
#############################
### Load the required package
#############################
library(bioanth)

#################################
### Load the Goldman Data Set ###
#################################
data(goldman, package = "bioanth")

###############################################
### Define an helper function for the plots ###
###############################################
plot_out_GDS <- function(data, pop, x_var = "RHML", y_var = "RTML",
                         title = NULL, index_outl = NULL) {
    ## Select a sub-sample from 'data':
    samp <- subset(data, NOTE == pop)
    ## Select complete cases for two variables:
    samp <- na.omit(samp[ , c(x_var, y_var)])
    ## Plot linear regression:
    form <- as.formula(paste(y_var, "~", x_var))
    plot(form, data = samp, pch = 16, main = title,
         xlab = paste(x_var, "(mm)"),
         ylab = paste(y_var, "(mm)"))
    abline(lm(form, data = samp), lty = 2)
    abline(lm(form, data = samp[-index_outl, ]),
           lty = 3, col = "blue")
}

############
### Plot ###
############
## Set graphial parameters:
par(mfrow = c(1, 3), cex = 0.9)
## Type 1: extreme residual value near the average of X
plot_out_GDS(data = goldman, pop = "Tsugumo Shell Mound",
           x_var = "RHML", y_var = "RTML",
           title = "(1) Tsugumo Shell Mound",
           index_outl = 8)
## Type 2: extreme individual on the X axis
plot_out_GDS(data = goldman, pop = "Germany, Hamann-Todd",
           x_var = "LFML", y_var = "RFML",
           title = "(2) Germany, Hamann-Todd",
           index_outl = c(15, 21))
## Type 3: leverage point
plot_out_GDS(data = goldman, pop = "Dynastic Egyptian, El Hesa",
           x_var = "RTML", y_var = "RFML",
           title = "(3) Dynastic Egyptian, El Hesa",
           index_outl = 23)
#+end_src

#+CAPTION: Illustration of three types of outliers in linear regression, with three different population samples drawn the Goldman Data Set. Their corresponding shortcodes in this dataset are indicated as the main title; the shortcodes of the variables are indicated as axes labels. The black dashed lines are the regression lines including all the individuals; the blue dotted lines are the regression lines excluding the visual outliers. label:fig:type_outliers_reg
#+ATTR_LATEX: :width \textwidth
#+RESULTS:
[[file:figures/type_outliers_reg.png]]

In a regression model, the leverage individuals of the type seen in Figure ref:fig:type_outliers_reg (3) are the most problematic. Leverage individuals can be identified through their high value of Cook's distance, which is provided as a standard diagnostic in most statistical software. A reasonable rule-of-thumb---that should be avoided in the case of a very small sample size---is that leverage points have a Cook's distance greater than 1 citep:cornillon2010_RegressionAvec.

However, it should be noted that robust methods for correlation and regression do exist citep:rousseeuw1987_RobustRegressionOutlier. Manually excluding outliers is not mandatory with those modern techniques, that have their own built-in way to handle outliers. 

A robust version of the correlation coefficient automatically restricts the computation to the "most central" part of the data, using the same minimum covariance determinant algorithm as the robust Mahalanobis distance detailed in section [[Robust Mahalanobis distance]] (Fig. ref:fig:robust-corr). In particular, potential outliers can be lefted in on the plots, thus allowing to discuss some particular cases without introducing any bias in the computation.

#+begin_src R :results file graphics :file figures/robust-correlation.png :exports results :width 400 :height 400 :tangle ./../R/Figure9_robust_corr.R
##############################
### Load required packages ###
##############################
library(bioanth)
library(mvoutlier)

#################################
### Load the Goldman Data Set ###
#################################
data(goldman)

##############################################
### Select the population sample "El Hesa" ###
##############################################
hesa <- subset(goldman, NOTE == "Dynastic Egyptian, El Hesa")
hesa <- na.omit(hesa[ , c("RTML", "RFML")])

###########################################
### Compute and plot robust correlation ###
###########################################
corr.plot(x = hesa$RTML, y = hesa$RFML,
          alpha = 0.05, quan = 3/4,
          xlab = "RTML (mm)", ylab = "RFML(mm)",
          pch = 16, asp = 1)
#+end_src

#+CAPTION: Classical and robust estimates of the correlation coefficient between the maximal lengths of the right humerus and femur within the population sample "Dynastic Egyptian, El Hesa" drawn from the Goldman Data Set. Correlation ellipsoids are given an $\alpha$ level of 0.95, and a proportion $h=3/4$ of individuals is used for MCD estimation. label:fig:robust-corr
#+ATTR_LATEX: :width 0.5\textwidth
#+RESULTS:
[[file:figures/robust-correlation.png]]

Robust alternatives for linear regression are also implemented in various R packages. The function ~MASS::rlm()~ implements an algorithm that gives different weights to the individuals according to their distance to the regression line, and iteratively re-fits the model until convergence citep:venables2010_ModernAppliedStatistics. Another option is the quantile regression citep:koenker2005_QuantileRegressionRoger, implemented in the function ~quantreg::rq()~, that replaces the mean by the median wihtin the framework of least squares estimation. As shown on Figure ref:fig:robust-regression, those two methods are usually consistent with each other, and with an ordinary linear regression performed after excluding the potential outliers.

#+begin_src R :results file graphics :file figures/quantile-regression.png :exports results :width 450 :height 450 :tangle ./../R/Figure10_robust_lm.R
##############################
### Load required packages ###
##############################
library(bioanth)
library(MASS)
library(quantreg)

#################################
### Load the Goldman Data Set ###
#################################
data(goldman)
### Select the population sample "El Hesa":
hesa <- subset(goldman, NOTE == "Dynastic Egyptian, El Hesa")
hesa <- na.omit(hesa[ , c("RTML", "RFML")])

###################
### Scatterplot ###
###################
par(cex = 1.12, mar = c(4, 4, 1, 1))
plot(RFML ~ RTML, data = hesa, asp = 1,
     xlab = "RTML (mm)", ylab = "RFML (mm)")
## 1. Usual OLS regression line (with outlier):
abline(lm(RFML ~ RTML, data = hesa), lty = 2)
## 2. Usual OLS regression line (without outlier):
abline(lm(RFML ~ RTML, data = hesa[-23, ]), col = "black")
## 3. Robust regression:
abline(rlm(RFML ~ RTML, data = hesa), col = "red")
## 4. Quantile regression:
abline(rq(RFML ~ RTML, data = hesa), col = "blue")
## Add legend:
legend("topleft", lty = c(2, 1, 1, 1), col = c("black", "black", "red", "blue"),
       legend = c("OLS (with outlier)",
                  "OLS (discarding outlier)",
                  "Robust iterative regression",
                  "Quantile regression"))         
#+end_src

#+CAPTION: Comparison of four strategies of linear regression between the right maximum femur and tibia lengths, using the population sample "Dynastic Egyptian, El Hesa" from the Goldman Data Set. Two OLS (ordinary least squares, i.e classical) linear regressions are performed, including or not the clear outlier. Two variants of robust regression are performed with the whole sample, including the outlier. label:fig:robust-regression
#+ATTR_LATEX: :width 0.5\textwidth
#+RESULTS:
[[file:figures/quantile-regression.png]]

** General case: the bagplot
Depending on the aim and context of the study, the two extreme points on the middle panel of Figure ref:fig:type_outliers_reg can be seen as clear outliers (they are exceedingly tall and short compared to the other individuals from this population sample) or not (they do respect the relationship between the two measurements). In other words, they are clearly outliers as regards their measurements, but are not outliers in the framework of a regression model.

When one only searches for outliers in a two-dimensional distribution---outside of the context of linear regression or correlation---the bagplot citep:rousseeuw1999_BagplotBivariateBoxplot is the appropriate tool. The bagplot is a bivariate generalization of the boxplot. An inner polygon (/bag/) contains about 50% of the individuals which are the closest to the bivariate sample median; an exterior /fence/ allows to identify the outliers and is defined by inflating the bag by a factor 3; and an intermediate region (the /loop/) is the convex hull of the outermost individuals that are not outliers. Rarely used in archaeological sciences---cite:oconnell2012_DietbodyOffsetHuman and cite:emery2018_MappingOriginsImperial are two of the few recent instances---, the bagplot provides a simple and visual way to identify bivariate outliers by an /ad-hoc/ rule (Fig. ref:fig:bagplot).

#+begin_src R :results file graphics :file figures/bagplot.png :exports results :width 600 :height 480 :tangle ./../R/Figure11_bagplot.R
##############################
### Load required packages ###
##############################
library(aplpack)
library(bioanth)
library(FactoMineR)

#################################
### Load the Goldman Data Set ###
#################################
data(goldman, package = "bioanth")
## Select a subsample of individuals (Delaware pop. sample):
goldman <- as.data.frame(goldman[ , c("NOTE", "RTMLD", "RTML")])
goldman <- na.omit(subset(goldman, NOTE == "Delaware"))
rownames(goldman) <- 1:nrow(goldman) # relabel the rows

######################
### Draw a bagplot ###
######################
par(mar = c(4.5, 4.5, 1, 1), cex = 1.15)
bagplot(x = goldman$RTMLD, y = goldman$RTML,
        na.rm = TRUE, cex = 1.25,
        xlab = "RTMLD (mm)", ylab = "RTML (mm)",
        show.center = FALSE, show.whiskers = FALSE)
set.seed(201909) # set seed to ensure reproducibility
autoLab(x = goldman$RTMLD, y = goldman$RTML,
        labels = rownames(goldman), cex = 1.1)
#+end_src

#+CAPTION: Bagplot for the the maximal length and medio-lateral diameter of the right tibia, measured on the population sample of Delaware (US-NJ, 500 BP) from the Goldman Data Set. label:fig:bagplot
#+ATTR_LATEX: :width 0.6\textwidth
#+RESULTS:
[[file:figures/bagplot.png]]
* Discussion and conclusion
As stated by [[citet:leys2019_HowClassifyDetect][::p. 5]], "/there are no universal rules to tell you when to consider a value as ‘too far’ from the others; researchers need to make this decision for themselves/". This statement is in line with cite:tukey1977_ExploratoryDataAnalysis work: outliers are data points flagged as somewhat unusual, and then constitute /candidates/ for being true---informative---anomalies. Detecting outliers should always lead to reflection, sometimes action, but the final interpretation and conclusion is up to the researcher.

Therefore, any method of outlier detection comes with several arbitrary choices. The constant $k$ in equations \eqref{eq:formula_loc_scale_univ} to \eqref{eq:adjusted_boxplot} strongly impacts the severity of the decision rule by narrowing or widening the "credibility intervals"; a similar role is played by the $\alpha$ level in equations \eqref{eq:maha} and \eqref{eq:robust_maha} for Mahalanobis distances. By choosing lower or higher values for such parameters, either only the clearest extreme values or even slightly unusual values will be regarded as outliers. It is not possible to give a universal recommendation to set those parameters at a given value, and the researcher should be prepared to defend the strategy of outlier detection adopted in a study.

Furthermore, it is rather unlikely that an archaeologist can know beforehand the distribution of the variable(s) considered in the underlying population. The gaussian distribution, or at least a symmetric distribution, can be a reasonable assumption in the majority of situations encountered in archaeological sciences. However, one can almost never know with certainty which distribution a given set of values comes from, and this may be a good reason to use modern methods that makes few or even no assumption on the distribution of the data, such as isolation forests.

For all those reasons, outlier detection is strongly user-dependent, and the strategy adopted should be explicitly stated: in some ambiguous situations (cf. Fig. ref:fig:asymGiza), the assumptions made by the researcher may strongly affect the results of outlier detection. Therefore, one should not rely on vague and non-specific assertions such as "after removing four outliers, we performed linear regression [...]" without additional details.

Applying several robust methods of outliers detection and comparing their results may also appear as a good practice. In rather simple cases (normally distributed data with sufficient sample size and moderate number of variables), they should lead to the same conclusions (as in Figures ref:fig:stripcharts-maha and ref:fig:anomaly_scores_sayala). When dealing with more complex patterns (e.g. involving nonlinear relationships, multimodal or asymmetric distributions), some discordance may appear, calling for an even more careful inspection of the data and of the potential candidates. The different methods of outliers detection all search for different types of outliers, and finding ways to compare them is an active topic in statistical research [[citep:unwin2019_MultivariateOutliersO3][e.g.,::]]. In the multivariate case, robust Mahalanobis distances and isolation forests may be seen as complementary, and can be used in combination, since they have truly different approaches. Indeed, the first method searches for unusual observations in a parametric model assuming roughly multivariate normal data (so that it delivers a ``yes/no'' answer at a given decision threshold). Conversely, isolation forests rank all individuals in terms of ``outlyingness'', without making any assumption about the distribution, and does not provide any definitive answer about any individual: it is up to the researcher to inspect carefully the individuals ``flagged'' by the algorithm, and to make a decision using his or her subjective knowledge.

The recent DDC algorithm may be very helpful in this latest step, by providing a complete map of deviating cells. Those entries may be either strong univariate anomalies or slightly odd combinations of variables. This method is maximally useful when dealing with high-dimensional datasets, both because of its internal logic---that takes advantage of the intercorrelation of the variables---and because it may become hard to understand why an individual is detected by Mahalanobis distance or isolation forests when the number of variables does not allow simple graphical representations anymore. In such a case, the DDC algorithm considerably helps the researcher to identify why some individuals may be regarded as outliers thanks to a very clear and synthetic graphical output (Fig. ref:fig:ddc_dsp2). It should also be noted that this algorithm is improved at a considerable pace, and several of its extensions citep:raymaekers2019_FlaggingHandlingCellwise,hubert2019_MacroPCAAllinOnePCA should be extremely valuable in osteology, since they allow both outlier detection and imputation of missing values.

Finally, it should be noted that categorical variables might also be considered when performing outlier detection, either by using algorithms which natively handle them (such as ``hdoutliers''), or by turning them manually into multivariate numeric values via correspondence analysis citep:unwin2019_MultivariateOutliersO3.

The focus of the present article was on outlier detection, and not outlier management in a broad sense. The problem of knowing what to do with the individuals that are detected as outliers is extensively covered in cite:leys2019_HowClassifyDetect. However, numerous robust methods have built-in way to handle outliers, and do not need a controversial manual exclusion. This article focused on robust correlation and regression methods, but most popular methods do have a robust equivalent which offers a valuable alternative for "contaminated data". Among other examples, robust principal component analysis citep:candes2011_RobustPrincipalComponent or robust estimation and hypothesis testing citep:wilcox2012_IntroductionRobustEstimation can be cited. Within the field of robust estimation, winsorization---i.e., replacing all the values exceeding a given threshold $t$ by the value $t$ itself---or trimming---i.e., removing a given percentage of the most extreme values in both directions---could be valuable tools in archaeology, and would offer some new ways to deal with outlying values in statistical inference.

* Acknowledgments
  :PROPERTIES:
  :UNNUMBERED: t
  :END:
I would like to thank Jaroslav Bruzek (University of Bordeaux, France) for allowing me to use part of the DSP2 reference sample in this study.

My warm thanks to Sabrina Granger (Urfist Bordeaux, France), who strongly contributed to put me on the path of reproducible research. The welcoming community of Emacs and Org-mode users helped me to solve some problems encountered while writing this manuscript. Arnaud Legrand (University of Grenoble 1, France) also gave me useful advice about Org-mode.

Finally, the two anonymous reviewers must be acknowledged for providing invaluable and very detailed comments to improve the manuscript, its general structure, and its ability to be fully reproduced. I learned very much from their suggestions. Readers can access the first version of the manuscript on GitLab (https://gitlab.com/f-santos/reproducibility-package-for-santos-2020-jasr) and, by comparing it to the present text, appreciate the significant improvements made thanks to the reviewers' comments.

* Data availability statement
  :PROPERTIES:
  :UNNUMBERED: t
  :END:
No new data were created in this study. However, all the datasets used within the text are freely available online, and are the property of their respective authors.

The Goldman Osteometric Data Set is available at \url{https://web.utk.edu/~auerbach/GOLD.htm}, and those data have been collected by Benjamin Auerbach. This dataset is also included in the R package ~bioanth~ citep:eanes2015_BioanthDatasetsUseful, and this is the source used in this study.

The DSP2 reference sample has been collected by Jaroslav Bruzek and is available in cite:bruzek2017_ValidationReliabilitySex as Supporting Information online. This dataset is also included in the R package ~anthrostat~, and this is the source used in this study.

\appendix
* Formulae of robust scale estimates for univariate outliers detection
label:appendix:rob-scale-estimate
Full mathematical details are given here for three possible robust scale estimates $\hat{s}$ which can be used as input in equation \eqref{eq:formula_loc_scale_univ} for univariate outliers detection.

** The interquartile range
The interquartile range (IQR) is defined by the difference between the third and first quartiles of the data. It can be shown that, for a gaussian distribution, $\hat{s} = IQR / a$, with a scale factor $a \approx 1.349$, is a consistent estimate of $\sigma$ citep:wan2014_EstimatingSampleMean. Therefore, in this first alternative, the outliers are those extreme values falling outside of the range $[m - k \cdot \frac{IQR}{1.349} \, ; \, m + k \cdot \frac{IQR}{1.349}]$.

** The median absolute deviation
The median absolute deviation (MAD) provides another estimate of $\sigma$ which is even more robust than the IQR citep:rousseeuw1993_AlternativesMedianAbsolute. For a given sample $x$, the MAD is defined as the scaled median of absolute deviations from the sample median:
#+begin_export latex
\begin{equation}
MAD = b \times \med (|x_i - \med(x)|_{1 \leq i \leq n})  \label{eq:mad}
\end{equation}
#+end_export
The scale factor $b$ depends on the underlying distribution of the data. If the normality assumption is reasonable (disregarding some potential extreme values), $b$ should be set to $1.4826$, which is approximately the opposite of the third theoretical quartile of the distribution $\mathcal{N}(0,1)$. With this method, the outliers are defined as those values that fall outside of the range $[m - k \cdot MAD \, ; \, m + k \cdot MAD]$

** The $S_n$ estimator
A third alternative is the $S_n$ estimator citep:rousseeuw1993_AlternativesMedianAbsolute. $S_n$ is defined by:
#+begin_export latex
\begin{equation}
S_n = c \cdot \med_i \left\{ \med_j |x_i - x_j| \right\}  \label{eq:sn}
\end{equation}
#+end_export
and is a very robust estimate of the $\sigma$ parameter of a gaussian distribution if the scale factor $c$ is set to $1.1926$. As for the two previous methods, the outliers are defined as those values that fall outside of the range $[m - k \cdot S_n \, ; \, m + k \cdot S_n]$

* Theoretical details for robust Mahalanobis distance
label:appendix:robust-maha
This method relies on the concept of generalized variance citep:wilks1960_MultidimensionalStatisticalScatter,sengupta2006_GeneralizedVariance, which is a measure of multivariate dispersion defined by the determinant of the covariance matrix, $|\Sigma|$. The robust Mahalanobis distance proceeds by iteratively drawing at random $h$ out of the $n$ individuals (with $h \in [n/2, n[$), and finally selecting the subsample of size $h$ that has the minimum generalized variance. Therefore, this can be seen as using only the "good part" of the data---i.e. a ``central'' part which does not include the potential outliers---to derive robust location and variability estimates. This best subsample of size $h$ is finally used to compute the sample estimates $\hat{\mu}_{\text{MCD}}$ and $\hat{\Sigma}_{\text{MCD}}$ that define the robust Mahalanobis distance:

#+begin_export latex
\begin{equation}
R_i = \sqrt{{}^t(x_i - \hat{\mu}_\text{MCD}) \, \hat{\Sigma}_\text{MCD}^{-1} \, (x_i - \hat{\mu}_\text{MCD})} \label{eq:robust_maha}
\end{equation}
#+end_export

The choice the parameter $h$ (i.e. the proportion of ``good data'' used to compute the robust estimators) may have a substantial impact when dealing with small samples. As a general advice, $h$ should be chosen with respect to the anticipated proportion of outliers in the study: if the researcher expects at least one fifth of outliers in his or her sample, $h$ should be less than $4n/5$ to avoid that contaminated data participate to the calculations. A study by cite:leys2018_DetectingMultivariateOutliers showed that choosing $h = 3n/4$ should be convenient in most situations, and offers a good compromise between robustness and accuracy. This is the value used in the present article.

* R packages used in this study
label:appendix:r-packages
As well as R 3.6.3 itself, the following R packages were used for writing this manuscript:
- ~anthrostat~ 0.1.5 citep:santos2020_AnthrostatSetUseful
- ~aplpack~ 190512 citep:wolf2019_AplpackAnotherPlot
- ~bioanth~ 0.1.0 citep:eanes2015_BioanthDatasetsUseful
- ~cellWise~ 2.1.1 citep:raymaekers2020_CellWiseAnalyzingData
- ~FactoMineR~ 2.3 citep:le2008_FactoMineRPackageMultivariate
- ~MASS~ 7.3-51.5 citep:venables2010_ModernAppliedStatistics
- ~mvoutlier~ 2.0.9 citep:filzmoser2018_MvoutlierMultivariateOutlier
- ~quantreg~ 5.55 citep:koenker2020_QuantregQuantileRegression
- ~robustbase~ 0.93.6 citep:todorov2009_ObjectOrientedFrameworkRobust
- ~scatterplot3d~ 0.3-41 citep:ligges2003_Scatterplot3dPackageVisualizing
- ~solitude~ 0.2.1 citep:srikanth2019_SolitudeImplementationIsolation
- ~univOutl~ 0.1-5 citep:dorazio2019_UnivOutlDetectionUnivariate

This exact computational environment is made publicly available through a Docker image that also includes Emacs 26.3, Org-mode 9.3.6, various other Emacs packages, and a LaTeX distribution. This ensures that the manuscript can be reproduced in its exact form on any computer, using the source Org file.

Full details are available on the GitLab repository  (https://gitlab.com/f-santos/reproducibility-package-for-santos-2020-jasr).

* References                                                         :ignore:
bibliography:complete_biblio.bib
